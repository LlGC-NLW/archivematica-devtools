#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# This file is part of the Archivematica development tools.
#
# Copyright 2010-2016 Artefactual Systems Inc. <http://artefactual.com>
#
# Archivematica is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Archivematica is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Archivematica.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import print_function
import sys

from elasticsearch import Elasticsearch
import elasticsearch.helpers

sys.path.append('/usr/lib/archivematica/archivematicaCommon')
import storageService as storage_service
from elasticSearchFunctions import getElasticsearchServerHostAndPort

conn = Elasticsearch(hosts=getElasticsearchServerHostAndPort())

query = {
    'aggs': {'transfer_uuids': {'terms': {'field': 'sipuuid', 'size': 0}}},
    'size': 0,
}

results = conn.search(
    body=query,
    index='transfers',
    doc_type='transferfile',
)

uuids = [bucket['key'] for bucket in results['aggregations']['transfer_uuids']['buckets']]

for transfer_uuid in uuids:
    query = {
        'query': {
            'term': {
                'sipuuid': transfer_uuid,
            }
        }
    }
    # A scan produces an unsorted set of all records matching the query,
    # which can be iterated over using multiple queries without redoing
    # the search. This provides us a way to reliably fetch all files
    # within a given transfer.
    # The elasticsearch.helpers.scan function is a helper that abstracts
    # this by providing an iterator that iterates over every record in the
    # set.
    iterator = elasticsearch.helpers.scan(
        conn,
        query=query,
        # The amount of time the record set should remain in memory
        # on the ES server; the operation needs to be finished within
        # this time. 15m is probably generous even for large transfers.
        scroll='15m',
        index='transfers',
        doc_type='transferfile',
    )
    # Produce a flat, in-memory list from the iterator
    record_set = [r['_source'] for r in iterator]

    print("Removing existing file records for transfer {}".format(transfer_uuid))
    try:
        storage_service.remove_files_from_transfer(transfer_uuid)
    except Exception as e:
        if e.response.status_code == 404:
            print("No transfer found for UUID \"{}\" (not present in this Storage Service?); not indexing files".format(transfer_uuid), file=sys.stderr)
            continue
    print("Indexing {} files in transfer {}...".format(len(record_set), transfer_uuid))
    storage_service.index_backlogged_transfer_contents(transfer_uuid, record_set)
